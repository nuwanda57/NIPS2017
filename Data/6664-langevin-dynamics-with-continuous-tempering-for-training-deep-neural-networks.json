{"1" : "C.Chen, D.Carlson, Z. Gan, C. Li, and L. Carin. Bridging the gap between stochastic gradient MCMC and stochastic optimization. In AISTATS, 2016",
"2" : "C. Chen, N. Ding, and L.Carin. On the convergence of stochastic gradient MCMC algorithms with high-order integrators. In NIPS, 2015",
"3" : "T. Chen, E. B. Fox, and C. Guestrin. Stochastic gradient Hamiltonial Monte Carlo. In Proceedings of the 31st International Conference on Machine Learning, pages 1683-1691, 2014",
"4" : "A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer networks. In AISTATS, 2015",
"5" : "Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y.Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS, 2014",
"6" : "S. Geman and C. Hwang. Diffusions for global optimization. SIAM Journal on Control and Optimization, 24(5):1031-1043, 1986",
"7" : "Xavier Glorot and Yosua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Aistats, volume 9, pages 249-256, 2010",
"8" : "G. Gobbo and B. J. Leimkuhler. Extended hamiltonian approach to continuous tempering. Physical Review E, 91(6):061301, 2015",
"9" : "I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016",
"10" : "L. Ingber. Simulates annealing: Practice versus theory. Mathematical and computer modelling, 18(11):29-57, 1993",
"11" : "Andrej Karpathy, Justin Johnson, and Fei-Fei Li. Visualizing and understanding recurrent networks. CoRR, abs/1506.02078, 2015",
"12" : "K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing Systems 29, pages 586-594, 2016",
"13" : "N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016",
"14" : "D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014",
"15" : "S. Kirpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science, 220(4598):671-680, 1983",
"16" : "A. Laio and M. Parrinello. Escaping free-energy minima. Proceeding of the National Academy of Science, 99(20):12562-12566, 2002",
"17" : "N. Lenner and G. Mathias. Continuous tempering molecular dynamics: A deterministic approach to simulated tempering. Journal of Chemical Theory and Computation, 12(2):486-498, 2016",
"18" : "J. C. Mattingly, A. M. Stuart, and M. V. Tretyakov. Convergence of numerical time-averaging and stationary measures via poisson equations. SIAM Journal on Numerical Analysis, 48(2):552-577, 2010",
"19" : "A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach, and J. Martens. Adding gradient noise improves learning for very deep networks. airXiv:1511.06807, 2015",
"20" : "D. C. Rapaport. The art of molecular dynamics simulation. Cambridge university press, 2004",
"21" : "A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In ICLR, 2014",
"22" : "I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton. On the importance of initialization and momentum in deep learning. ICML, 2013",
"23" : "P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res., 11:3371-3408, 2010",
"24" : "M. Welling and Y. W. Tech. Bayesian learning via stochastic gradient langevin dynamics. In ICML., 2011",
"25" : "S. Zhang, A. Choromanska, and Y.LeCun. Deep learning with elastic averaging SGD. In NIPS, 2015"}
